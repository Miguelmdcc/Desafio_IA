{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando o dataset com temporal_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.forecasting.model_selection import temporal_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seller_id = 100000060\n",
    "# time_serie = model_df[model_df['Seller ID'] == seller_id]\n",
    "# time_serie.drop(columns=['Seller ID','Day of Week','Last Week','Last Month'],inplace=True)\n",
    "# time_serie['Change'] = time_serie['Change'].astype('Int64')\n",
    "# time_serie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag de um dia na nossa time serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_serie.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = time_serie.pop('Transactioned')\n",
    "# y = pd.DataFrame(data=target,columns=['Transactioned'])\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_serie = time_serie.shift(1)\n",
    "# time_serie.drop(pd.to_datetime('2024-01-30'), inplace=True)\n",
    "# time_serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_to_pred = pd.to_datetime('2024-11-04')\n",
    "# time_serie = time_serie[time_serie.index < date_to_pred]\n",
    "# y = y[y.index<date_to_pred]\n",
    "# y.drop(pd.to_datetime('2024-01-30'), inplace=True)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test, X_train, X_test = temporal_train_test_split(y,time_serie,test_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.forecasting.base import ForecastingHorizon\n",
    "# import numpy as np\n",
    "\n",
    "# fh = ForecastingHorizon(np.arange(1,len(y_test)+2),is_relative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # Calculando scale_pos_weight\n",
    "# n_pos = len(y_train[y_train['Transactioned'] == 1])\n",
    "# n_neg = len(y_train[y_train['Transactioned'] == 0])\n",
    "# scale_pos_weight = n_neg / n_pos\n",
    "\n",
    "# xgb_model = XGBRegressor(\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=5,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     scale_pos_weight=scale_pos_weight\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.forecasting.compose import make_reduction\n",
    "\n",
    "# forecaster = make_reduction(xgb_model, window_length=12, strategy=\"recursive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecaster.fit(y=y_train,X=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = forecaster.predict(fh=fh,X=X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.utils.plotting import plot_series\n",
    "# plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Análise de erros\n",
    "# errors = abs(y_test['Transactioned'].values - y_pred.values)\n",
    "# print(\"Erro Médio Absoluto (MAE):\", errors.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterando em sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from xgboost import XGBRegressor\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.utils.plotting import plot_series\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sktime.forecasting.model_selection import SlidingWindowSplitter,ForecastingGridSearchCV, ForecastingOptunaSearchCV\n",
    "from sktime.split import SingleWindowSplitter\n",
    "from tqdm import tqdm\n",
    "from sktime.performance_metrics.forecasting import MeanAbsoluteError\n",
    "from optuna import Trial\n",
    "import optuna\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/workspace/Desafio Grão Direto IA/Data/model_df.xlsx'\n",
    "model_df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pca(df):\n",
    "#     # Define the features variable\n",
    "#     features = df.columns.tolist()\n",
    "\n",
    "#     # 1. Normalização das features\n",
    "#     scaler = StandardScaler()\n",
    "#     df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "#     # 2. Aplicar PCA\n",
    "#     pca = PCA()\n",
    "#     pca.fit(df_scaled)\n",
    "\n",
    "#     # 3. Variância explicada\n",
    "#     explained_variance_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "#     # 4. Determinar o número ideal de componentes para atingir, por exemplo, 80% da variância\n",
    "#     threshold = 0.80\n",
    "#     num_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "#     print(f'Número de componentes para atingir {threshold*100:.0f}% da variância: {num_components}')\n",
    "\n",
    "#     # 5. Visualizar variância explicada\n",
    "#     plt.figure(figsize=(12, 7))\n",
    "#     plt.bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, alpha=0.6, color='g', \n",
    "#             label='Individual Explained Variance')\n",
    "#     plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o', linestyle='-', color='r', \n",
    "#             label='Cumulative Explained Variance')\n",
    "#     plt.axhline(y=threshold, color='b', linestyle='--', label=f'{threshold*100:.0f}% Variance Threshold')\n",
    "#     plt.axvline(x=num_components, color='purple', linestyle='--', label=f'{num_components} Components')\n",
    "#     plt.xlabel('Principal Components')\n",
    "#     plt.ylabel('Explained Variance')\n",
    "#     plt.title('Explained Variance by Principal Components')\n",
    "#     plt.xticks(range(1, len(explained_variance_ratio)+1))\n",
    "#     plt.legend(loc='upper left')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "#     loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "#                             index=df.columns)\n",
    "#     loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "#                             index=features)\n",
    "\n",
    "#     # Exibir os loadings mais importantes para os principais componentes\n",
    "#     print(\"Loadings (Pesos das Features nos Componentes Principais):\")\n",
    "#     print(loadings)\n",
    "\n",
    "#     # Soma absoluta dos loadings para os 4 primeiros PCs\n",
    "#     top_n_pcs = 4  # Quantos PCs você quer considerar\n",
    "#     sum_abs_loadings = loadings.iloc[:, :top_n_pcs].abs().sum(axis=1)\n",
    "\n",
    "#     # Adiciona a soma como uma nova coluna no DataFrame original das features\n",
    "#     loadings['Sum_Absolute_Loadings_Top4'] = sum_abs_loadings\n",
    "\n",
    "#     # Ordena as features por importância (soma decrescente)\n",
    "#     sorted_features = loadings['Sum_Absolute_Loadings_Top4'].sort_values(ascending=False)\n",
    "\n",
    "#     # Exibir as features mais importantes\n",
    "#     print(\"Soma Absoluta dos Pesos nos 4 Primeiros PCs (por Feature):\")\n",
    "#     print(sorted_features)\n",
    "\n",
    "#     # Identificar as features mais importantes para os primeiros componentes principais\n",
    "#     top_features = {}\n",
    "#     for i in range(num_components):\n",
    "#         pc_loadings = loadings[f'PC{i+1}'].abs().sort_values(ascending=False)\n",
    "#         top_features[f'PC{i+1}'] = pc_loadings.head(3).index.tolist()\n",
    "\n",
    "#     print(\"\\nFeatures mais importantes para cada componente principal:\")\n",
    "#     for pc, features in top_features.items():\n",
    "#         print(f'{pc}: {features}')\n",
    "\n",
    "# pca_df = model_df[model_df['Seller ID'] == 100000060]\n",
    "# pca_df.set_index('Date',inplace=True)\n",
    "# pca_df = pca_df.drop(columns=['Transactioned','Seller ID',])\n",
    "\n",
    "# pca(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seller_gs(seller_id):\n",
    "    #print(f\"Processando Seller ID: {seller_id}\")\n",
    "    \n",
    "    # Filtrar os dados para o Seller ID atual\n",
    "    time_serie = model_df[model_df['Seller ID'] == seller_id]\n",
    "    time_serie = time_serie.drop(columns=['Seller ID','Day of Week'])\n",
    "    \n",
    "    # Preparação da série temporal\n",
    "    time_serie.set_index('Date', inplace=True)\n",
    "    target = time_serie.pop('Transactioned')\n",
    "    y = pd.DataFrame(data=target, columns=['Transactioned'])\n",
    "    time_serie = time_serie.shift(1)\n",
    "    \n",
    "    # Remover valores fora do intervalo de datas desejado\n",
    "    date_to_pred = pd.to_datetime('2024-11-04')\n",
    "    first_date = pd.to_datetime('2024-01-30')\n",
    "    mask = (time_serie.index > first_date) & (time_serie.index < date_to_pred)\n",
    "    time_serie = time_serie.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    # Dividir os dados em treino e teste\n",
    "    y_train, y_test, X_train, X_test = temporal_train_test_split(y, time_serie, test_size=2)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train = pd.DataFrame(data=X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(data=X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # Configuração do Forecasting Horizon\n",
    "    fh = ForecastingHorizon([3], is_relative=True)\n",
    "    \n",
    "    # Calculando scale_pos_weight\n",
    "    n_pos = len(y_train[y_train['Transactioned'] == 1])\n",
    "    n_neg = len(y_train[y_train['Transactioned'] == 0])\n",
    "    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1\n",
    "    \n",
    "    # Configuração do modelo e busca\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 500],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 6],\n",
    "        'subsample': [0.1, 0.3],\n",
    "        'colsample_bytree': [0.6, 0.8]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "    \n",
    "    # Criar o modelo\n",
    "    forecaster = make_reduction(xgb_model, window_length=12, strategy=\"recursive\")\n",
    "\n",
    "    cv = SingleWindowSplitter(window_length=len(y)-2,fh=2)\n",
    "\n",
    "    # Criar o ForecastingGridSearchCV\n",
    "    grid_search = ForecastingGridSearchCV(\n",
    "        forecaster=forecaster,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=MeanAbsoluteError()\n",
    "    )\n",
    "\n",
    "    #Treinando modelo\n",
    "    grid_search.fit(y=y_train, X=X_train)\n",
    "    \n",
    "    # Previsão\n",
    "    y_pred = grid_search.predict(fh=fh, X=X_test)\n",
    "    # y_pred_04 = y_pred.loc['2024-11-04']\n",
    "\n",
    "    # # Análise de erros\n",
    "    # errors = abs(y_test['Transactioned'].values - y_pred.values)\n",
    "    # mae = errors.mean()\n",
    "    # print(f\"Erro Médio Absoluto (MAE) para Seller ID {seller_id}: {mae}\")\n",
    "    \n",
    "    # # Plot das séries\n",
    "    # plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "    # plt.title(f\"Previsão para Seller ID: {seller_id}\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    return{\n",
    "        \"Seller ID\": seller_id,\n",
    "        \"Date\": y_pred.index.values[0],  # Extract the single date value\n",
    "        \"Score\": y_pred['Transactioned'].values[0],  # Extract the single score value\n",
    "        # \"MAE\": mae\n",
    "        \"Params\": grid_search.cv_results_\n",
    "    }\n",
    "\n",
    "# Lista de Seller IDs\n",
    "seller_ids = model_df['Seller ID'].unique()\n",
    "\n",
    "# Resultados para armazenar os erros por vendedor\n",
    "results = []\n",
    "\n",
    "# Criar a barra de progresso manualmente\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Usar tqdm para iterar nos vendedores enquanto processa\n",
    "    for result in tqdm(executor.map(process_seller_gs, seller_ids), total=len(seller_ids), desc=\"Processando Sellers\"):\n",
    "        results.append(result)\n",
    "\n",
    "# Criar DataFrame com os resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72 minutos e 33,5 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seller_optuna(seller_id):\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    #print(f\"Processando Seller ID: {seller_id}\")\n",
    "    \n",
    "    # Filtrar os dados para o Seller ID atual\n",
    "    time_serie = model_df[model_df['Seller ID'] == seller_id]\n",
    "    time_serie = time_serie.drop(columns=['Seller ID','Day of Week'])\n",
    "    \n",
    "    # Preparação da série temporal\n",
    "    time_serie.set_index('Date', inplace=True)\n",
    "    target = time_serie.pop('Transactioned')\n",
    "    y = pd.DataFrame(data=target, columns=['Transactioned'])\n",
    "    time_serie = time_serie.shift(1)\n",
    "    \n",
    "    # Remover valores fora do intervalo de datas desejado\n",
    "    date_to_pred = pd.to_datetime('2024-11-04')\n",
    "    first_date = pd.to_datetime('2024-01-30')\n",
    "    mask = (time_serie.index > first_date) & (time_serie.index < date_to_pred)\n",
    "    time_serie = time_serie.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    # Dividir os dados em treino e teste\n",
    "    y_train, y_test, X_train, X_test = temporal_train_test_split(y, time_serie, test_size=2)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train = pd.DataFrame(data=X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(data=X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # Configuração do Forecasting Horizon\n",
    "    fh = ForecastingHorizon([3], is_relative=True)\n",
    "    \n",
    "    # Calculando scale_pos_weight\n",
    "    n_pos = len(y_train[y_train['Transactioned'] == 1])\n",
    "    n_neg = len(y_train[y_train['Transactioned'] == 0])\n",
    "    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1\n",
    "    \n",
    "    # Configuração do modelo e busca\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 500],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 6],\n",
    "        'subsample': [0.1, 0.3],\n",
    "        'colsample_bytree': [0.6, 0.8]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "    \n",
    "    # Criar o modelo\n",
    "    forecaster = make_reduction(xgb_model, window_length=12, strategy=\"recursive\")\n",
    "\n",
    "    cv = SingleWindowSplitter(window_length=len(y)-2,fh=2)\n",
    "\n",
    "    # Criar o ForecastingGridSearchCV\n",
    "    grid_search = ForecastingOptunaSearchCV(\n",
    "        forecaster=forecaster,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_evals=10,\n",
    "    )\n",
    "\n",
    "    #Treinando modelo\n",
    "    grid_search.fit(y=y_train, X=X_train)\n",
    "    \n",
    "    # Previsão\n",
    "    y_pred = grid_search.predict(fh=fh, X=X_test)\n",
    "    # y_pred_04 = y_pred.loc['2024-11-04']\n",
    "\n",
    "    # # Análise de erros\n",
    "    # errors = abs(y_test['Transactioned'].values - y_pred.values)\n",
    "    # mae = errors.mean()\n",
    "    # print(f\"Erro Médio Absoluto (MAE) para Seller ID {seller_id}: {mae}\")\n",
    "    \n",
    "    # # Plot das séries\n",
    "    # plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "    # plt.title(f\"Previsão para Seller ID: {seller_id}\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    return{\n",
    "        \"Seller ID\": seller_id,\n",
    "        \"Date\": y_pred.index.values[0],  # Extract the single date value\n",
    "        \"Score\": y_pred['Transactioned'].values[0],  # Extract the single score value\n",
    "        # \"MAE\": mae\n",
    "        \"Params\": grid_search.cv_results_\n",
    "    }\n",
    "\n",
    "# Lista de Seller IDs\n",
    "seller_ids = model_df['Seller ID'].unique()\n",
    "\n",
    "# Resultados para armazenar os erros por vendedor\n",
    "results = []\n",
    "\n",
    "# Criar a barra de progresso manualmente\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Usar tqdm para iterar nos vendedores enquanto processa\n",
    "    for result in tqdm(executor.map(process_seller_optuna, seller_ids), total=len(seller_ids), desc=\"Processando Sellers\"):\n",
    "        results.append(result)\n",
    "\n",
    "# Criar DataFrame com os resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = results_df['Score'].mean()\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_that_made_transaction_on_target_date_04 = [\n",
    "    100000060,\n",
    "    100000141,\n",
    "    100000192,\n",
    "    100000310,\n",
    "    100000348,\n",
    "    100001098,\n",
    "    100001362,\n",
    "    100001525,\n",
    "    100002035,\n",
    "    100002271,\n",
    "    100002332,\n",
    "    100002339,\n",
    "    100002402,\n",
    "    100002426,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_that_made_transaction_on_target_date_05 = [\n",
    "    100000180,\n",
    "    100000037,\n",
    "    100001254,\n",
    "    10000088,\n",
    "    100001203,\n",
    "    100001098,\n",
    "    100001599,\n",
    "    100001885,\n",
    "    100000608,\n",
    "    100000151,\n",
    "    100000638,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_200_predictions = results_df.sort_values(\n",
    "    ascending=False, by=\"Score\"\n",
    ").head(200)\n",
    "\n",
    "\n",
    "first_200_predictions = first_200_predictions[\n",
    "    first_200_predictions[\"Seller ID\"].isin(users_that_made_transaction_on_target_date_04)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_200_predictions.to_excel('C:/workspace/Desafio Grão Direto IA/Data/params_gscv.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coisas a fazer\n",
    "- PCA |\n",
    "- GRIDSEARCH\n",
    "- OPTUNASEARCH\n",
    "- CONCURRENT FUTURES V\n",
    "- Hierarchical Clustering |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomar cuidado com features categoricas, até mesmo transformadas em int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model_cl = model_df.drop(columns=['Seller ID', 'Transactioned'])\n",
    "# model_cl = model_cl.set_index('Date')\n",
    "# scaler = StandardScaler()\n",
    "# model_clp = scaler.fit_transform(model_cl)\n",
    "# model_clp = pd.DataFrame(model_clp, columns=model_cl.columns)\n",
    "# model_clp = model_clp.transpose()\n",
    "\n",
    "# linked = linkage(model_clp, method='ward', metric='euclidean')\n",
    "\n",
    "# df_linked = pd.DataFrame(linked, columns=['c1', 'c2', 'distance', 'size'])\n",
    "# df_linked[['c1', 'c2', 'size']] = df_linked[['c1', 'c2', 'size']].astype('Int64')\n",
    "\n",
    "# plt.figure(figsize=(15,5))\n",
    "\n",
    "# dendrogram(linked, orientation='top', labels=model_clp.index, distance_sort='descending', show_leaf_counts=True)\n",
    "# plt.xlabel('Features')\n",
    "# plt.ylabel('Wards')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estudoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
