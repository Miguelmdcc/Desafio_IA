{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mercado = 'C:/workspace/Desafio Grão Direto IA/Data/mercado-desafio-tratado.xlsx'\n",
    "path_transacoes = 'C:/workspace/Desafio Grão Direto IA/Data/transações-desafio-tratado.xlsx'\n",
    "\n",
    "mercado = pd.read_excel(path_mercado)\n",
    "transacoes =  pd.read_excel(path_transacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_transactioned_sum = transacoes.groupby('Seller ID')['Transactioned'].sum()\n",
    "seller_ids = seller_transactioned_sum[seller_transactioned_sum > 2].index\n",
    "transacoes = transacoes[transacoes['Seller ID'].isin(seller_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.merge(transacoes,mercado,on='Date',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando o dataset com temporal_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.forecasting.model_selection import temporal_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seller_id = 100000060\n",
    "# time_serie = model_df[model_df['Seller ID'] == seller_id]\n",
    "# time_serie.drop(columns=['Seller ID','Day of Week','Last Week','Last Month'],inplace=True)\n",
    "# time_serie['Change'] = time_serie['Change'].astype('Int64')\n",
    "# time_serie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag de um dia na nossa time serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_serie.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = time_serie.pop('Transactioned')\n",
    "# y = pd.DataFrame(data=target,columns=['Transactioned'])\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_serie = time_serie.shift(1)\n",
    "# time_serie.drop(pd.to_datetime('2024-01-30'), inplace=True)\n",
    "# time_serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_to_pred = pd.to_datetime('2024-11-04')\n",
    "# time_serie = time_serie[time_serie.index < date_to_pred]\n",
    "# y = y[y.index<date_to_pred]\n",
    "# y.drop(pd.to_datetime('2024-01-30'), inplace=True)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test, X_train, X_test = temporal_train_test_split(y,time_serie,test_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.forecasting.base import ForecastingHorizon\n",
    "# import numpy as np\n",
    "\n",
    "# fh = ForecastingHorizon(np.arange(1,len(y_test)+2),is_relative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # Calculando scale_pos_weight\n",
    "# n_pos = len(y_train[y_train['Transactioned'] == 1])\n",
    "# n_neg = len(y_train[y_train['Transactioned'] == 0])\n",
    "# scale_pos_weight = n_neg / n_pos\n",
    "\n",
    "# xgb_model = XGBRegressor(\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=5,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     scale_pos_weight=scale_pos_weight\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.forecasting.compose import make_reduction\n",
    "\n",
    "# forecaster = make_reduction(xgb_model, window_length=12, strategy=\"recursive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecaster.fit(y=y_train,X=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = forecaster.predict(fh=fh,X=X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sktime.utils.plotting import plot_series\n",
    "# plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Análise de erros\n",
    "# errors = abs(y_test['Transactioned'].values - y_pred.values)\n",
    "# print(\"Erro Médio Absoluto (MAE):\", errors.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterando em sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from xgboost import XGBRegressor\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.utils.plotting import plot_series\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pca(df):\n",
    "#     # Define the features variable\n",
    "#     features = df.columns.tolist()\n",
    "\n",
    "#     # 1. Normalização das features\n",
    "#     scaler = StandardScaler()\n",
    "#     df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "#     # 2. Aplicar PCA\n",
    "#     pca = PCA()\n",
    "#     pca.fit(df_scaled)\n",
    "\n",
    "#     # 3. Variância explicada\n",
    "#     explained_variance_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "#     # 4. Determinar o número ideal de componentes para atingir, por exemplo, 80% da variância\n",
    "#     threshold = 0.80\n",
    "#     num_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "#     print(f'Número de componentes para atingir {threshold*100:.0f}% da variância: {num_components}')\n",
    "\n",
    "#     # 5. Visualizar variância explicada\n",
    "#     plt.figure(figsize=(12, 7))\n",
    "#     plt.bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, alpha=0.6, color='g', \n",
    "#             label='Individual Explained Variance')\n",
    "#     plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o', linestyle='-', color='r', \n",
    "#             label='Cumulative Explained Variance')\n",
    "#     plt.axhline(y=threshold, color='b', linestyle='--', label=f'{threshold*100:.0f}% Variance Threshold')\n",
    "#     plt.axvline(x=num_components, color='purple', linestyle='--', label=f'{num_components} Components')\n",
    "#     plt.xlabel('Principal Components')\n",
    "#     plt.ylabel('Explained Variance')\n",
    "#     plt.title('Explained Variance by Principal Components')\n",
    "#     plt.xticks(range(1, len(explained_variance_ratio)+1))\n",
    "#     plt.legend(loc='upper left')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "#     loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "#                             index=df.columns)\n",
    "#     loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "#                             index=features)\n",
    "\n",
    "#     # Exibir os loadings mais importantes para os principais componentes\n",
    "#     print(\"Loadings (Pesos das Features nos Componentes Principais):\")\n",
    "#     print(loadings)\n",
    "\n",
    "#     # Soma absoluta dos loadings para os 4 primeiros PCs\n",
    "#     top_n_pcs = 4  # Quantos PCs você quer considerar\n",
    "#     sum_abs_loadings = loadings.iloc[:, :top_n_pcs].abs().sum(axis=1)\n",
    "\n",
    "#     # Adiciona a soma como uma nova coluna no DataFrame original das features\n",
    "#     loadings['Sum_Absolute_Loadings_Top4'] = sum_abs_loadings\n",
    "\n",
    "#     # Ordena as features por importância (soma decrescente)\n",
    "#     sorted_features = loadings['Sum_Absolute_Loadings_Top4'].sort_values(ascending=False)\n",
    "\n",
    "#     # Exibir as features mais importantes\n",
    "#     print(\"Soma Absoluta dos Pesos nos 4 Primeiros PCs (por Feature):\")\n",
    "#     print(sorted_features)\n",
    "\n",
    "#     # Identificar as features mais importantes para os primeiros componentes principais\n",
    "#     top_features = {}\n",
    "#     for i in range(num_components):\n",
    "#         pc_loadings = loadings[f'PC{i+1}'].abs().sort_values(ascending=False)\n",
    "#         top_features[f'PC{i+1}'] = pc_loadings.head(3).index.tolist()\n",
    "\n",
    "#     print(\"\\nFeatures mais importantes para cada componente principal:\")\n",
    "#     for pc, features in top_features.items():\n",
    "#         print(f'{pc}: {features}')\n",
    "\n",
    "# pca_df = model_df[model_df['Seller ID'] == 100000060]\n",
    "# pca_df.set_index('Date',inplace=True)\n",
    "# pca_df = pca_df.drop(columns=['Transactioned','Seller ID',])\n",
    "\n",
    "# pca(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seller(seller_id):\n",
    "    print(f\"Processando Seller ID: {seller_id}\")\n",
    "    \n",
    "    # Filtrar os dados para o Seller ID atual\n",
    "    time_serie = model_df[model_df['Seller ID'] == seller_id]\n",
    "    time_serie = time_serie.drop(columns=['Seller ID','Close','CBOT','Last Week','Last Month','Last Transaction','Price','Change','Percentage Change'])\n",
    "    \n",
    "    # Preparação da série temporal\n",
    "    time_serie.set_index('Date', inplace=True)\n",
    "    target = time_serie.pop('Transactioned')\n",
    "    y = pd.DataFrame(data=target, columns=['Transactioned'])\n",
    "    time_serie = time_serie.shift(1)\n",
    "    \n",
    "    # Remover valores fora do intervalo de datas desejado\n",
    "    date_to_pred = pd.to_datetime('2024-11-04')\n",
    "    first_date = pd.to_datetime('2024-01-30')\n",
    "    mask = (time_serie.index > first_date) & (time_serie.index < date_to_pred)\n",
    "    time_serie = time_serie.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    # Dividir os dados em treino e teste\n",
    "    y_train, y_test, X_train, X_test = temporal_train_test_split(y, time_serie, test_size=20)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train = pd.DataFrame(data=X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(data=X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # Configuração do Forecasting Horizon\n",
    "    fh = ForecastingHorizon(pd.DatetimeIndex([date_to_pred]), is_relative=False)\n",
    "    \n",
    "    # Calculando scale_pos_weight\n",
    "    n_pos = len(y_train[y_train['Transactioned'] == 1])\n",
    "    n_neg = len(y_train[y_train['Transactioned'] == 0])\n",
    "    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1\n",
    "    \n",
    "    # Configurar o XGBRegressor\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "    \n",
    "    # Criar e treinar o modelo\n",
    "    forecaster = make_reduction(xgb_model, window_length=12, strategy=\"recursive\")\n",
    "    forecaster.fit(y=y_train, X=X_train)\n",
    "    \n",
    "    # Previsão\n",
    "    y_pred = forecaster.predict(fh=fh, X=X_test)\n",
    "    # y_pred_04 = y_pred.loc['2024-11-04']\n",
    "    \n",
    "    # Análise de erros\n",
    "    errors = abs(y_test['Transactioned'].values - y_pred.values)\n",
    "    mae = errors.mean()\n",
    "    print(f\"Erro Médio Absoluto (MAE) para Seller ID {seller_id}: {mae}\")\n",
    "    \n",
    "    # # Plot das séries\n",
    "    # plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "    # plt.title(f\"Previsão para Seller ID: {seller_id}\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    return{\n",
    "        \"Seller ID\": seller_id,\n",
    "        \"Date\": y_pred.index.values[0],  # Extract the single date value\n",
    "        \"Score\": y_pred['Transactioned'].values[0],  # Extract the single score value\n",
    "        \"MAE\": mae\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de Seller IDs\n",
    "seller_ids = model_df['Seller ID'].unique()\n",
    "\n",
    "# Resultados para armazenar os erros por vendedor\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_seller,seller_ids))\n",
    "\n",
    "# Criar DataFrame com os resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = results_df['MAE'].mean()\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_that_made_transaction_on_target_date = [\n",
    "    100000060,\n",
    "    100000141,\n",
    "    100000192,\n",
    "    100000310,\n",
    "    100000348,\n",
    "    100001098,\n",
    "    100001362,\n",
    "    100001525,\n",
    "    100002035,\n",
    "    100002271,\n",
    "    100002332,\n",
    "    100002339,\n",
    "    100002402,\n",
    "    100002426,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_200_predictions = results_df.sort_values(\n",
    "    ascending=False, by=\"Score\"\n",
    ").head(200)\n",
    "\n",
    "\n",
    "first_200_predictions[\n",
    "    first_200_predictions[\"Seller ID\"].isin(users_that_made_transaction_on_target_date)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coisas a fazer\n",
    "- PCA\n",
    "- GRIDSEARCH\n",
    "- CONCURRENT FUTURES V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estudoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
